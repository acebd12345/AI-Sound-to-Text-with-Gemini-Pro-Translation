import os
import json
import traceback
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from faster_whisper import WhisperModel
from google.cloud import storage

app = FastAPI()

# --- å…¨åŸŸæ¨¡å‹è¼‰å…¥ (Cold Start æ™‚åŸ·è¡Œä¸€æ¬¡) ---
# ä½¿ç”¨ large-v3-turbo æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä¸”ç²¾åº¦é«˜
# device="cuda" æ˜¯é—œéµï¼Œæ²’æœ‰ GPU é€™è¡Œæœƒå ±éŒ¯
print("æ­£åœ¨è¼‰å…¥ Whisper æ¨¡å‹ (Large-v3-turbo)...")
try:
    # å¾æ˜ åƒæª”ä¸­é å…ˆä¸‹è¼‰çš„ç›®éŒ„è¼‰å…¥æ¨¡å‹
    model_path = "model"
    print(f"å¾ {model_path} è¼‰å…¥æ¨¡å‹...")
    model = WhisperModel(model_path, device="cuda", compute_type="float16")
    print("æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
except Exception as e:
    print(f"æ¨¡å‹è¼‰å…¥å¤±æ•— (è«‹æª¢æŸ¥æ˜¯å¦æœ‰ GPU ç’°å¢ƒ): {e}")
    model = None

storage_client = storage.Client()

@app.get("/health")
async def health_check():
    return {"status": "ok", "model_loaded": model is not None}

@app.post("/")
async def handle_event(request: Request):
    """
    Eventarc è§¸ç™¼å…¥å£ï¼š
    ç•¶ GCS æœ‰æ–°æª”æ¡ˆå¯«å…¥æ™‚ï¼ŒGCP æœƒå°‡äº‹ä»¶è³‡æ–™ POST åˆ°é€™è£¡ã€‚
    """
    print("æ”¶åˆ°è§¸ç™¼è«‹æ±‚...")

    if model is None:
        print("éŒ¯èª¤ï¼šWhisper æ¨¡å‹æœªè¼‰å…¥ï¼Œç„¡æ³•è™•ç†è«‹æ±‚")
        return JSONResponse(
            status_code=503,
            content={"status": "error", "message": "Whisper model not loaded. GPU environment may be unavailable."}
        )

    # 1. è§£æ CloudEvent è³‡æ–™
    event = await request.json()
    
    # å…¼å®¹æ€§è™•ç†ï¼šEventarc çš„è³‡æ–™çµæ§‹æœ‰æ™‚åœ¨ bodyï¼Œæœ‰æ™‚åœ¨ message.data
    bucket_name = None
    file_name = None

    try:
        if 'bucket' in event:
            bucket_name = event['bucket']
            file_name = event['name']
        elif 'message' in event and 'data' in event['message']:
            # Pub/Sub æ ¼å¼éœ€è¦ base64 è§£ç¢¼ (å¾ˆå°‘è¦‹ï¼Œä½†ä»¥é˜²è¬ä¸€)
            import base64
            decoded = base64.b64decode(event['message']['data']).decode('utf-8')
            data_json = json.loads(decoded)
            bucket_name = data_json['bucket']
            file_name = data_json['name']
        
        if not bucket_name or not file_name:
            print("é GCS äº‹ä»¶æˆ–è§£æå¤±æ•—ï¼Œç•¥éã€‚")
            return {"status": "ignored"}

        # é˜²æ­¢ç„¡é™è¿´åœˆçš„é‡è¦éæ¿¾
        # æˆ‘å€‘åªè™•ç† 'raw_audio/' è³‡æ–™å¤¾å…§çš„æª”æ¡ˆ
        # å¦‚æœæ˜¯ 'transcripts/' æˆ–å…¶ä»–è³‡æ–™å¤¾ï¼Œç›´æ¥å¿½ç•¥
        if "raw_audio/" not in file_name:
            print(f"è·³éé raw_audio æª”æ¡ˆ: {file_name}")
            return {"status": "skipped"}

        # è·³é metadata.jsonï¼Œå®ƒä¸æ˜¯éŸ³è¨Šæª”æ¡ˆ
        if file_name.endswith("metadata.json"):
            print(f"è·³é metadata æª”æ¡ˆ: {file_name}")
            return {"status": "skipped"}

        print(f"é–‹å§‹è™•ç†æª”æ¡ˆ: gs://{bucket_name}/{file_name}")

        # è§£æ file_id èˆ‡ chunk_index
        path_parts = file_name.split('/')
        file_id = path_parts[-2] if len(path_parts) >= 3 else "unknown"
        chunk_index = path_parts[-1] if len(path_parts) >= 3 else os.path.basename(file_name)

        # 2. ä¸‹è¼‰æª”æ¡ˆåˆ°æš«å­˜å€ (/tmp)ï¼Œç”¨ file_id + chunk_index ç¢ºä¿è·¯å¾‘å”¯ä¸€
        local_input_path = f"/tmp/{file_id}_{chunk_index}"
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(file_name)
        blob.download_to_filename(local_input_path)

        # é è¨­åƒæ•¸ (Speech)
        use_vad = True
        temp = 0.2
        prompt_text = "ä»¥ä¸‹æ˜¯ç¹é«”ä¸­æ–‡çš„å­—å¹•ã€‚"

        # è®€å– Metadata èª¿æ•´åƒæ•¸
        try:
            meta_blob = bucket.blob(f"raw_audio/{file_id}/metadata.json")
            if meta_blob.exists():
                meta = json.loads(meta_blob.download_as_text())
                if meta.get("mode") == "song":
                    print("ğŸµ æ¨¡å¼åµæ¸¬: æ­Œæ›² (VAD=False, Temp=0)")
                    use_vad = False
                    temp = 0
                    prompt_text = "ä»¥ä¸‹æ˜¯ç¹é«”ä¸­æ–‡çš„æ­Œè©ã€‚"
        except Exception as e:
            print(f"Metadata è®€å–å¤±æ•— (ä½¿ç”¨é è¨­å€¼): {e}")

        # 3. åŸ·è¡Œ GPU è½‰éŒ„
        # beam_size=1 æœ€å¿«ï¼›beam_size=5 è¼ƒæº–ã€‚é€™è£¡ç”¨ 1 è¿½æ±‚æ¥µé€Ÿ
        # language="zh": å¼·åˆ¶ä¸­æ–‡ï¼Œé¿å…äº‚è·³èªè¨€
        # condition_on_previous_text=False: é¿å…é‡è¤‡ä¸Šä¸€å¥ (é¬¼æ‰“ç‰†)
        # word_timestamps=True: æé«˜æ™‚é–“è»¸ç²¾æº–åº¦
        segments, info = model.transcribe(
            local_input_path,
            beam_size=5,
            vad_filter=use_vad,
            vad_parameters=dict(min_silence_duration_ms=500),
            initial_prompt=prompt_text,
            language="zh",
            condition_on_previous_text=False,
            word_timestamps=True,
            temperature=temp
        )

        # 4. æ•´ç†çµæœ
        full_text = ""
        segment_list = []
        for segment in segments:
            full_text += segment.text
            segment_list.append({
                "start": segment.start,
                "end": segment.end,
                "text": segment.text
            })

        # 5. ä¸Šå‚³çµæœåˆ° GCS (transcripts è³‡æ–™å¤¾)
        # æª”åè½‰æ›: raw_audio/xyz/1 -> transcripts/xyz_part_1.json
        # file_id å’Œ chunk_index å·²åœ¨ä¸Šæ–¹è§£æ
        if len(path_parts) >= 3:
            result_blob_name = f"transcripts/{file_id}_part_{chunk_index}.json"
        else:
            safe_name = file_name.replace('/', '_')
            result_blob_name = f"transcripts/{safe_name}.json"

        result_data = {
            "text": full_text,
            "segments": segment_list,
            "duration": info.duration
        }

        output_blob = bucket.blob(result_blob_name)
        output_blob.upload_from_string(
            json.dumps(result_data, ensure_ascii=False),
            content_type="application/json"
        )
        
        print(f"è½‰éŒ„æˆåŠŸï¼å·²å„²å­˜è‡³: {result_blob_name}")

        # æ¸…ç†
        os.remove(local_input_path)
        return {"status": "success", "output": result_blob_name}

    except Exception as e:
        error_msg = traceback.format_exc()
        print(f"è™•ç†ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)